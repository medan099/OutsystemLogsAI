{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e931cc0a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T23:41:54.819374Z",
     "iopub.status.busy": "2025-03-13T23:41:54.818943Z",
     "iopub.status.idle": "2025-03-13T23:42:03.725135Z",
     "shell.execute_reply": "2025-03-13T23:42:03.724002Z"
    },
    "papermill": {
     "duration": 8.912855,
     "end_time": "2025-03-13T23:42:03.727358",
     "exception": false,
     "start_time": "2025-03-13T23:41:54.814503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import re\n",
    "import shutil\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a217f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T23:42:03.735517Z",
     "iopub.status.busy": "2025-03-13T23:42:03.734957Z",
     "iopub.status.idle": "2025-03-13T23:43:39.786815Z",
     "shell.execute_reply": "2025-03-13T23:43:39.785574Z"
    },
    "papermill": {
     "duration": 96.0593,
     "end_time": "2025-03-13T23:43:39.790532",
     "exception": false,
     "start_time": "2025-03-13T23:42:03.731232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les ressources nécessaires pour NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6673ee6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T23:43:39.797974Z",
     "iopub.status.busy": "2025-03-13T23:43:39.797505Z",
     "iopub.status.idle": "2025-03-13T23:43:39.805544Z",
     "shell.execute_reply": "2025-03-13T23:43:39.804404Z"
    },
    "papermill": {
     "duration": 0.013977,
     "end_time": "2025-03-13T23:43:39.807512",
     "exception": false,
     "start_time": "2025-03-13T23:43:39.793535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Supprimer les espaces inutiles\n",
    "        text = re.sub(r'\\d+', '', text)  # Supprimer les nombres\n",
    "        text = text.lower()\n",
    "\n",
    "        doc = self.nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]  # Lemmatisation et suppression des stopwords\n",
    "        \n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94ef09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T23:43:39.815486Z",
     "iopub.status.busy": "2025-03-13T23:43:39.815053Z",
     "iopub.status.idle": "2025-03-13T23:43:39.823068Z",
     "shell.execute_reply": "2025-03-13T23:43:39.821779Z"
    },
    "papermill": {
     "duration": 0.014062,
     "end_time": "2025-03-13T23:43:39.824887",
     "exception": false,
     "start_time": "2025-03-13T23:43:39.810825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " \"\"\"Classe responsable de l'extraction et de l'enregistrement des images\n",
    "    contenues dans les posts du forum sous forme de Base64.\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import base64\n",
    "\n",
    "class ImageProcessor:\n",
    "\n",
    "    def __init__(self, output_dir):\n",
    "        \"\"\"\n",
    "        Initialise l'instance du processeur d'images.\n",
    "\n",
    "        :param output_dir: Répertoire où les images extraites seront enregistrées.\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        # Supprime le dossier de sortie s'il existe déjà, puis le recrée\n",
    "        if os.path.exists(output_dir):\n",
    "            shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def save_images(self, base64_images, prefix):\n",
    "        \"\"\"\n",
    "        Enregistre les images en Base64 sous forme de fichiers PNG.\n",
    "\n",
    "        :param base64_images: Liste des images encodées en Base64.\n",
    "        :param prefix: Préfixe pour le nom des fichiers enregistrés.\n",
    "        :return: Liste des noms de fichiers des images enregistrées.\n",
    "        \"\"\"\n",
    "        filenames = []\n",
    "\n",
    "        for idx, base64_img in enumerate(base64_images):\n",
    "            try:\n",
    "                # Décodage de l'image Base64\n",
    "                img_data = base64.b64decode(base64_img)\n",
    "                img_filename = f\"{prefix}_{idx}.png\"\n",
    "                img_path = os.path.join(self.output_dir, img_filename)\n",
    "\n",
    "                # Écriture du fichier image\n",
    "                with open(img_path, \"wb\") as img_file:\n",
    "                    img_file.write(img_data)\n",
    "\n",
    "                filenames.append(img_filename)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur lors du décodage de l'image {idx}: {e}\")\n",
    "\n",
    "        return filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4051bb35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T23:43:39.832523Z",
     "iopub.status.busy": "2025-03-13T23:43:39.832104Z",
     "iopub.status.idle": "2025-03-13T23:43:39.837365Z",
     "shell.execute_reply": "2025-03-13T23:43:39.836112Z"
    },
    "papermill": {
     "duration": 0.010826,
     "end_time": "2025-03-13T23:43:39.839107",
     "exception": false,
     "start_time": "2025-03-13T23:43:39.828281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Classe représentant un post du forum, incluant le titre, l'URL, \n",
    "    la description et les solutions associées.\"\"\"\n",
    "class Post:\n",
    "    def __init__(self, title, url, description, solutions):\n",
    "        self.title = title\n",
    "        self.url = url\n",
    "        self.description = description\n",
    "        self.solutions = solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927cab07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T23:43:39.846937Z",
     "iopub.status.busy": "2025-03-13T23:43:39.846511Z",
     "iopub.status.idle": "2025-03-13T23:53:43.998218Z",
     "shell.execute_reply": "2025-03-13T23:53:43.996794Z"
    },
    "papermill": {
     "duration": 604.164466,
     "end_time": "2025-03-13T23:53:44.006822",
     "exception": false,
     "start_time": "2025-03-13T23:43:39.842356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prétraitement terminé ! Données sauvegardées dans /kaggle/working/processed_forum_data_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "class ForumPostProcessor:\n",
    "    \"\"\"Classe responsable du prétraitement des posts de forum, y compris le nettoyage du texte et la gestion des images.\"\"\"\n",
    "\n",
    "    def __init__(self, input_file, output_file, output_dir):\n",
    "        \"\"\"\n",
    "        Initialise le processeur de posts du forum.\n",
    "\n",
    "        :param input_file: Chemin du fichier JSON d'entrée contenant les posts.\n",
    "        :param output_file: Chemin du fichier JSON de sortie pour stocker les posts traités.\n",
    "        :param output_dir: Répertoire où seront enregistrées les images extraites.\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.cleaner = TextCleaner()\n",
    "        self.image_processor = ImageProcessor(output_dir)\n",
    "        \n",
    "    def process_text_with_images(self, text, images):\n",
    "        \"\"\"\n",
    "        Remplace les références aux images sous forme de texte par leurs chemins de fichiers réels.\n",
    "\n",
    "        :param text: Texte contenant des références aux images (ex: \"[Image_0]\").\n",
    "        :param images: Liste des chemins des images correspondantes.\n",
    "        :return: Texte avec les références d'images remplacées par leurs chemins réels.\n",
    "        \"\"\"\n",
    "        image_pattern = re.compile(r\"\\[Image_(\\d+)\\]\")\n",
    "\n",
    "        def replace_match(match):\n",
    "            index = int(match.group(1))\n",
    "            return f\"[Image: {images[index]}]\" if index < len(images) else match.group(0)\n",
    "\n",
    "        return image_pattern.sub(replace_match, text)\n",
    "        \n",
    "    def process_post(self, post):\n",
    "        \"\"\"\n",
    "        Traite un post du forum en nettoyant son texte et en gérant ses images.\n",
    "\n",
    "        :param post: Dictionnaire contenant les données du post (titre, description, solutions...).\n",
    "        :return: Un objet Post contenant le post traité.\n",
    "        \"\"\"\n",
    "        description_text = post[\"description\"].get(\"text\", \"\") if isinstance(post[\"description\"], dict) else post[\"description\"]\n",
    "        description_images = post[\"description\"].get(\"images\", []) if isinstance(post[\"description\"], dict) else []\n",
    "        \n",
    "        description_text = self.cleaner.clean_text(description_text)\n",
    "        description_text = self.process_text_with_images(description_text, description_images)\n",
    "        \n",
    "        solutions = []\n",
    "        for sol_idx, solution in enumerate(post.get(\"solutions\", [])):\n",
    "            solution_text = self.cleaner.clean_text(solution[\"text\"])\n",
    "            solution_images = solution.get(\"images\", [])\n",
    "            extracted_images = self.image_processor.save_images(solution_images, f\"img_{sol_idx}\")\n",
    "            solution_text = self.process_text_with_images(solution_text, extracted_images)\n",
    "            solutions.append({\"text\": solution_text, \"images\": extracted_images})\n",
    "        \n",
    "        return Post(post[\"title\"], post[\"url\"], {\"text\": description_text, \"images\": description_images}, solutions)\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Lit le fichier JSON d'entrée, traite chaque post, puis enregistre les résultats dans un fichier JSON de sortie.\n",
    "        \"\"\"\n",
    "        with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "            forum_posts = json.load(f)\n",
    "        \n",
    "        processed_posts = [self.process_post(post).__dict__ for post in forum_posts]\n",
    "        \n",
    "        with open(self.output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(processed_posts, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"✅ Prétraitement terminé ! Données sauvegardées dans {self.output_file}\")\n",
    "\n",
    "# Exécution du traitement\n",
    "input_json_file = \"/kaggle/input/outsystemforums/processed_forum_data.json\"\n",
    "output_json_file = \"/kaggle/working/processed_forum_data_cleaned.json\"\n",
    "output_dir = \"/kaggle/working/images\"\n",
    "\n",
    "processor = ForumPostProcessor(input_json_file, output_json_file, output_dir)\n",
    "processor.process()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6788295,
     "sourceId": 10919189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6837594,
     "sourceId": 10986000,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 714.979224,
   "end_time": "2025-03-13T23:53:46.975744",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-13T23:41:51.996520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
